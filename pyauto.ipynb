{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:55:13.758112Z",
     "start_time": "2024-08-01T07:55:13.755664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image.flags.writeable = False                  \n",
    "    results = model.process(image)                 \n",
    "    image.flags.writeable = True                  \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    return image, results\n",
    "\n"
   ],
   "id": "c305ee878b8f4fc1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:59:03.071195Z",
     "start_time": "2024-08-01T07:58:48.762947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "def augment_frame(frame):\n",
    "    augmented_frames = []\n",
    "    # Flip horizontally\n",
    "    augmented_frames.append(cv2.flip(frame, 1))\n",
    "    # Rotate\n",
    "    rows, cols, _ = frame.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), 15, 1)\n",
    "    augmented_frames.append(cv2.warpAffine(frame, M, (cols, rows)))\n",
    "    # Add noise\n",
    "    noise = np.random.randint(0, 50, (rows, cols, 3), dtype='uint8')\n",
    "    augmented_frames.append(cv2.add(frame, noise))\n",
    "    return augmented_frames\n",
    "\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('Video/QIPEC.mp4')\n",
    "frames = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "cap.release()\n",
    "\n",
    "\n",
    "\n",
    "augmented_frames = []\n",
    "for frame in frames:\n",
    "    augmented_frames.extend(augment_frame(frame))\n",
    "\n",
    "\n",
    "# Process frames with mediapipe\n",
    "sequences = []\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for frame in augmented_frames:\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequences.append(keypoints)\n",
    "sequence_length = 50\n",
    "if len(sequences) < sequence_length:\n",
    "    # Pad sequences with zeros if less than 50\n",
    "    padding = np.zeros((sequence_length - len(sequences), sequences[0].shape[0]))\n",
    "    sequences = np.vstack((sequences, padding))\n",
    "else:\n",
    "    sequences = sequences[:sequence_length]\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'test'  \n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "\n",
    "for i, keypoints in enumerate(sequences):\n",
    "    np.save(os.path.join(data_path, f'{i}.npy'), keypoints)\n",
    "# Convert to numpy array and reshape\n",
    "sequences = np.array(sequences).reshape(-1, 50, 1662)"
   ],
   "id": "e30192d658f4feec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722499130.692760   33472 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1722499130.693887   43152 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.1), renderer: Mesa Intel(R) UHD Graphics (TGL GT1)\n",
      "W0000 00:00:1722499130.744728   43140 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722499130.757059   43146 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722499130.757637   43145 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722499130.757704   43144 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722499130.757937   43147 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722499130.762961   43143 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722499130.765962   43141 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722499130.769487   43140 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-01T07:59:32.690572Z",
     "start_time": "2024-08-01T07:59:32.687245Z"
    }
   },
   "source": "np.load(\"test/0.npy\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50906348,  0.2875011 , -0.60989499, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "246f31ff822e730c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:41:17.273660Z",
     "start_time": "2024-08-01T07:41:17.185532Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3480b4864438e4b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T13:19:57.131159Z",
     "start_time": "2024-07-31T13:19:57.129009Z"
    }
   },
   "outputs": [],
   "source": [
    "tts = gTTS(text=text, lang='vi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba4f82db5e8cbf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T13:20:20.563484Z",
     "start_time": "2024-07-31T13:20:19.774411Z"
    }
   },
   "outputs": [],
   "source": [
    "tts.save(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b95c65fe17f9749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T13:20:26.987923Z",
     "start_time": "2024-07-31T13:20:20.564919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
     ]
    }
   ],
   "source": [
    "from playsound import playsound\n",
    "playsound(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab6c451758310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
